from google.cloud import bigquery

def load_to_bigquery(event, context):
    # Initialize the BigQuery client
    client = bigquery.Client()

    # Extract the bucket name and file name from the event
    bucket_name = event['bucket']
    file_name = event['name']
    
    # Extract table name from file name (assuming file_name is in format tablename/yyyy/mm/dd/hh/*.csv)
    table_name = file_name.split('/')[0]  # This will be 'employees'
    
    # Construct the BigQuery table ID
    table_id = f"cloudside-academy.cloudside-academy.airbyte_task.{table_name}"
	
    # Load job configuration
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,  # If your CSV has headers
        autodetect=True,  # Automatically infer schema
    )

    # The URI of the file in GCS
    uri = f"gs://{bucket_name}/{file_name}"

    # Load the data from GCS into BigQuery
    load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)

    # Wait for the load job to complete
    load_job.result()

    print(f"Loaded {load_job.output_rows} rows into {table_id}")


