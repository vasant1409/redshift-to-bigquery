import os
from google.cloud import bigquery
from google.cloud import storage
import functions_framework
from google.api_core.exceptions import NotFound

# Triggered by a change in a storage bucket
@functions_framework.cloud_event
def hello_gcs(cloud_event):
    # Get data from the event
    data = cloud_event.data

    # Extract bucket and file name from the event data
    bucket_name = data["bucket"]
    file_name = data["name"]

    # Log event details
    print(f"Processing file: {file_name} in bucket: {bucket_name}")

    # Initialize BigQuery client
    client = bigquery.Client()
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    
    # Hardcoded dataset_id and project_id
    dataset_id = 'bck_stereo_sf'
    project_id = 'cloudside-academy'
    processed_files_table_id = f"{project_id}.{dataset_id}.processed_files"
    
    # Ensure that the processed_files table exists
    ensure_processed_files_table_exists(client, processed_files_table_id)

    # List all files in the bucket
    blobs = bucket.list_blobs()

    for blob in blobs:
        file_name = blob.name

        # Check if the file is in CSV or compressed CSV format
        if file_name.endswith('.csv.gz') or file_name.endswith('.gz'):
            # Extract table name from the file path
            table_name = file_name.split('/')[1].split('_')[0].lower()  # Adjust based on your file naming
            table_id = f"{project_id}.{dataset_id}.{table_name}"

            # Check if this file has already been processed
            if file_already_processed(file_name, client, processed_files_table_id):
                print(f"File {file_name} has already been processed. Skipping.")
                continue

            # Construct the GCS URI for the file
            uri = f'gs://{bucket_name}/{file_name}'

            # Configure the load job settings
            job_config = bigquery.LoadJobConfig(
                source_format=bigquery.SourceFormat.CSV,
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # Append mode
                skip_leading_rows=1,  # Assuming the first row is headers
                autodetect=True,      # Let BigQuery infer the schema
                max_bad_records=10,   # Allow up to 10 bad records
                ignore_unknown_values=True  # Ignore extra columns not in schema
            )

            # Load data into BigQuery table from GCS URI
            load_job = client.load_table_from_uri(
                uri,
                table_id,
                job_config=job_config
            )

            # Wait for the load job to complete
            load_job.result()

            print(f"File {file_name} successfully loaded into table {table_id}.")

            # Mark the file as processed
            mark_file_as_processed(file_name, client, processed_files_table_id)

        else:
            print(f"File {file_name} is not a CSV or gzipped CSV file. Skipping.")

def ensure_processed_files_table_exists(client, table_id):
    try:
        # Check if the table exists
        client.get_table(table_id)
        print(f"Table {table_id} exists.")
    except NotFound:
        # Create the table if it does not exist
        print(f"Table {table_id} not found. Creating it now.")
        schema = [
            bigquery.SchemaField("file_name", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("processed_at", "TIMESTAMP", mode="REQUIRED"),
        ]
        table = bigquery.Table(table_id, schema=schema)
        client.create_table(table)
        print(f"Table {table_id} created successfully.")

def file_already_processed(file_name, client, table_id):
    # Check if the file has already been processed
    query = f"""
    SELECT file_name FROM `{table_id}`
    WHERE file_name = @file_name
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("file_name", "STRING", file_name)
        ]
    )
    query_job = client.query(query, job_config=job_config)
    results = query_job.result()
    return results.total_rows > 0

def mark_file_as_processed(file_name, client, table_id):
    # Mark the file as processed by inserting a row into the processed_files table
    rows_to_insert = [
        {"file_name": file_name, "processed_at": bigquery.Timestamp(datetime.utcnow())}
    ]
    errors = client.insert_rows_json(table_id, rows_to_insert)
    if not errors:
        print(f"File {file_name} marked as processed.")
    else:
        print(f"Error marking file {file_name} as processed: {errors}")
